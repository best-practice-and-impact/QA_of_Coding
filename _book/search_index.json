[
["index.html", "QA of Code Preface", " QA of Code Joshua Halls 2020-03-31 Preface ALPHA This is a draft of Government Statistical Service guidance. It is unpublished and does not represent the views of the ONS or the GSS. Twitter: GSSGoodPractice, UKGSS Email us: gsshelp@statistics.gov.uk "],
["introduction.html", "Chapter 1 Introduction 1.1 Who is this guidance for? 1.2 What is its aim? 1.3 Why move away from spreadsheets? 1.4 Why move from SAS or SPSS to an open source tool? 1.5 Open source 1.6 For managers", " Chapter 1 Introduction This guidance is part of the quality assurance guidance published by the Best Practice and Impact team at the Office for National Statistics. This guidance has been created to support analysis professionals in government who use coding languages for in their work such as the production of statistics or modeling. 1.1 Who is this guidance for? This guidance is for analysts who are want to make their work reproducible and ensure that they use best practice. The aim of the guidance is to promote good practice in coding across the government analytical professions. It is aimed primarily at analysts who use code but have not been formally trained in computer science, rather than data scientists. The guidance provides an introduction to techniques and methods. It is not a comprehensive learning resource. However, it is also not an introduction to coding. You are will get the most from this guidance if you are familiar with coding principles, have used a script based language before, or currently use R or Python for analysis. At the end of chapters there are links to resources which provide a more comprehensive and in-depth guide to the methods we discuss. These are not the only learning resources available, but we have tried to link to ones that are used widely. 1.2 What is its aim? Analysis in government is moving away from legacy systems that are largely manual to workflows built using open source languages and tools that encourage rigorous documentation and a full audit trail. This trend, which builds on best practice in software engineering, provides opportunities for using new tools and methods to enhance the quality of the systems used to produce analysis. Many producers of such systems are not data scientists or coding experts, but other analysts or non-analytical staff who may not be familiar with these new approaches. This guidance will introduce some of the common practices that are recommended for implementing analysis in a transparent and fully auditable way and brings together best practice from across government. 1.3 Why move away from spreadsheets? Many departments are moving some analysis from spreadsheets to more automated workflows built with open source languages like R and Python. There are several reasons for this. Spreadsheet errors Spreadsheets intermesh data and code together. This makes it hard to quality assure and test the logic. It is mixed up with the data and it is harder to understand what the code is doing. Splitting the code and data allows us to write tests to make sure the code is doing what it is supposed to. Spreadsheets are not designed to work with best practice software testing. Errors are relatively common because of human error. This list of errors caused by spreadsheets illustrates the problem. One of the problems is the high degree of copy and pasting that using spreadsheets encourages, coupled with the intermeshing of data and logic together. It is estimated that 20 per cent of genetic research papers contains errors because a spreadsheet converted gene names into calendar dates. This demonstrates the problem of spreadsheet logic preforming tasks without informing the user. When this is combined with the difficulty of building proper tests, the resulting workflows are error prone. This, in turn, leads to the requirement for large amounts of time intensive, manual quality assurance. Lack of proper version control Whilst it is possible to version control spreadsheets, this is difficult and time consuming when compared to the version control functionality of tools like Git. These tools preserve a complete record of every change, along with the reasons and decisions behind those changes. This provides a complete audit trail and allows managers to have confidence in the quality assurance of the analysis without having to go through the work line by line. When using Git, it is very difficult to make changes without this being recorded. Along with its ease of use, this makes version control a fundamental part of the analytical workflow rather than relying on the analyst to remember to recorded changes and maintain proper version control manually. Spreadsheets are less reproducible Spreadsheets often involve lots of manual steps or ‘point and clicks’. These steps are not recorded, which makes reproducing work later difficult or impossible. Notes and documentation may help, but even detailed documentation is unlikely to give a comprehensive record of all the steps taken. Testing and QA often happens at the end, not throughout Quality assurance is time consuming when done manually and can take up a significant proportion of analyst time. Automating quality assurance and code management, alongside building testing into the workflow, frees up time to do more analysis and interpret the results. 1.4 Why move from SAS or SPSS to an open source tool? expand black box- can’t examine the algorithms hard to do Testing more limited cost money 1.5 Open source Open source languages like Python and R are increasing in popularity across government. One advantage of using these tools is that we can reduce the number of steps where the data needs to be moved from one program (or format) into another. This is in line with the principle of reproducibility set out in the government’s AQUA guidance on quality analysis in government, as the entire process can be set out in code, greatly reducing the likelihood of manual transcription errors. The Government Digital Service Technology Code of Practice is a set of criteria to help government design, build and buy technology. It’s a cross-government agreed standard in the spend controls process. Criterion 3 “Be open and use open source” recommends using open source languages to improve transparency, flexibility and accountability. It is easy to find learning resources and guides for open source languages, as they are supported by large and vibrant communities. This allows for a large degree of self learning, if an output, method or technique is widely used (and even if it is cutting edge), there is usually a library to support it already in place and a guide to teach it that is freely available for all. As a result of these large communities of users, open source languages can perform an enormous array of tasks. This means that departments can try innovative new approaches such as interactive dashboards or web scraping quickly and easily. Which language a department chooses will depend on business needs. In general, R and Python are the most widely used open source analysis tools and are good at similar tasks 1.6 For managers Managers who lead the production of analysis outputs like statistics or models are responsible for showing that those outputs are fit for purpose and properly quality assured. Whilst spreadsheet based workflows are familiar to all, moving to an open source workflow will require changes to the process. Because the tools and methods are often unfamiliar at the start, managers may be unsure of the robustness of the process. In consequence, processes might be slowed down or not deployed or the manager may feel they need to go line by line through the process. This guidance will support managers and explain what they need to do to be certain that best practice has been followed. If the process follows these best practices then a manager can have confidence in the output. Managers can ask whether business risks that they own have been properly tested and mitigated. Because workflows are fully audited and transparent, a manager can see every change and how the changes were managed. This allows the analysists to focus on the coding and the managers to focus on managing. "],
["reproducible-auditable-and-assured.html", "Chapter 2 Reproducible, Auditable and Assured 2.1 Reproducible 2.2 Auditable 2.3 Assured 2.4 What is a reproducible analytical pipeline? 2.5 Guidance 2.6 Strategies and approaches", " Chapter 2 Reproducible, Auditable and Assured When we do analysis it must be fit for purpose. If it isn’t, we risk misinforming decisions. Bad analysis can result in harm or misallocation of public funds. We must figure out the right steps to take to ensure high quality analysis. This book recognises three founding principles of good analysis, each supported by the one before. Reproducibility guarantees that we have done what we have said we have done, and that users could do it too. Auditability means that we know why we chose our analysis, and who is responsible for each bit - including assurance. Assurance improves the average quality and communicates that quality to users. 2.1 Reproducible Reproducibility is the only thing you can guarantee in your analysis. It is the first pillar of good analysis. If you can’t prove that you can run the same analysis, with the same data, and obtain the same results then you are not adding valuable analysis. The secondary assurances of peer review, rigorous testing, and validity are secondary to being able to reproduce any analysis that you produce in a proportionate amount of time. Reproducible analysis relies on a transparent production process so that anyone can follow our steps and understand our results. The transparency eases reuse of our methods and results. Easy reproducibility helps colleagues test and validate what we have done. As reproducibility is guaranteed, users and colleagues can focus on verifying that the implentation is correct and that the research is useful for its intended purpose. Reproducibility relies on effective documentation. Good documentation should show how our methodology and our implementation map to each other. Good documentation should allow users and other researchers to reuse and adapt our analysis. Reproducible analysis supports the requirements of the Code of Practice for Statistics around quality assurance and transparency (auditability). Wherever possible, we share the code we used to build the outputs, along with enough data to allow for proper testing. 2.2 Auditable If decisions are made based on analysis then we must make sure that the story of that analysis is available. Our analysis and the evidence we provide must be able to be held to scrutiny and audited. Auditable analysis is about being able to, at any point, answer: Who made which decisions at what time and based on what evidence? Answering these questions gives decision makers and users greater trust in our work. They know the story of our analysis and why we made certain analytical choices. They know who is responsible for each part of the analysis, including the assurance. They know exactly what changes have been made at any point. In a reproducible workflow, we must bring together the code and the data that we used to generate our results. These are ideally published alongside our reports, and alongside a record of analytical choices made and responsible owners of those choices. The transparency this gives our work helps trustworthiness. More eyes examining our work can point out challenges or flaws that can help us to improve. We can be fully open about the decisions we made when we generated our outputs so that other analysts can follow what we did and re-create them. By making our analysis reproducible, we make it easier for others to quality assure, assess and critique. 2.3 Assured Good quality analysis requires good quality assurance. If decisions are being made based on analysis then this analysis must be held to high standards However, some of the analysis that we do in government doesn’t bear on decisions at that level. We don’t want to overburden analysts with QA procedures when they aren’t required. In government we advocate proportionality - the right quality assurance procedures for the right analysis. Analysis can be proportionately assured through peer review and defensive design. We advocate following your department’s guidance on what proportionate quality assurance looks like. Most departments derive their guidance from the Aqua book. Assurance is best guaranteed through peer review. Peer reviewers must be able to understand your analytical choices and be able to reproduce your conclusions. Guarantees of quality assurance should be published alongside any report, or be taken into consideration by decision makers. 2.4 What is a reproducible analytical pipeline? Producing official statistics can be time-consuming and painstaking. We need to make sure that our outputs are both accurate and timely. Across government we aim to create effective and efficient analytical workflows which are repeatable over time and follow the principles of reproducible analysis. We call these Reproducible Analytical Pipelines (RAP). Reproducible analysis is still not widely practiced across government. Many analysts use proprietary (paid-for) analytical tools like SAS or SPSS in combination with programs like Excel, Word or Acrobat to create statistical products. The processes for creating statistics this way are usually manual or semi-manual. Colleagues typically repeat parts of the process manually to quality assure the outputs. This makes the analysis difficult to reproduce. This way of working is time consuming and can be frustrating because the manual steps can be quite hard to replicate quickly. These processes are also prone to error, because the input data and the outputs are not connected directly, only through the analyst’s manual intervention. More recently, the tools and techniques available to analysts have evolved. Open-source tools like Python and R, coupled with the use of version control and software management platforms like Git, used widely in software engineering to make it easier to collaborate, have made it possible to develop more automatic, streamlined processes, accompanied by a full audit trail. RAP was first piloted in the Government Statistical Service in 2017 by analysts in the Department for Digital, Culture, Media &amp; Sport (DCMS) and the Department for Education (DfE). They collaborated with data scientists from the Government Digital Service (GDS) to automate the production of statistical bulletins. To support the adoption of RAP across government, there is a network of RAP champions. Champions are responsible for promoting reproducible analysis and the use of reproducible analytical pipelines and supporting others who want to develop RAP in their own teams. 2.5 Guidance Aqua Book GSS Quality Strategy DCMS Data Ethics Framework Government Transformation Strategy Communicating quality, change and uncertainty 2.6 Strategies and approaches 2.6.1 One place 2.6.2 Checklists and processes Don’t reinvent the wheel. It’s always worth taking at look at what went well, and what could be improved, when you are at natural pauses. 2.6.3 Defensive design 2.6.4 RAP is Knowledge Management "],
["modelling.html", "Chapter 3 Modelling 3.1 AQUA book quality assurance principles for code 3.2 Repeatable 3.3 RAP Levels align with AQUA Levels", " Chapter 3 Modelling This figure sets out the principle that as models increase in business risk and complexity, the level of appropriate quality assurance also increases. This is similar to the idea that we can achieving different levels of reproducible analysis, which is explored in the next chapter. 3.1 AQUA book quality assurance principles for code The quality assurance principles from the AQUA book apply to all models, whether they are built in Excel or an open source language. The main difference is that new software practices allow analysts to enhance the quality assurance of models more makes the process of peer review easier and more efficient. It is also easy using Git to push the code used to produce models to external platforms such as GitHub allowing the model code to be publicity available. This enhancing the transparency of government and would allow analysts or informed citizens to contribute by finding error or bugs or suggesting new approaches. This is also a powerful tool for spreading best practice as analysts would be able to look at effective models from across government rather than keep them siloed in departments. Git offers an platform for effective peer review whether from others in the team but also from analysts in other teams or departments. Making peer review easier to preform is an effective tool for increasing overall QA. Git also tracks changes made to code and who made them, and includes the decisions behind the changes. This ensures that models have a full audit trail built in without analysts having to invest resources documenting it. Ensuring that a model is producing the correct output is a time intensive process of analytical testing. This process can be automated using software Testing frameworks. This enhances the robustness of models and increases confidence that the output is a result of the data not the process. 3.2 Repeatable The AQUA Book states that analysis should be repeatable to be considered valid. Using open source languages effectively should increase the repeatability of analysis when compared with existing spreadsheet methods. The latter often require many manual steps which can be hard to repeat. Using scripted languages means that the entire process has been written down and can be exactly repeated. While it is possible to implement some of these ideas in proprietary tools such as SPSS or SAS, the proprietary nature of the software languages means a license is needed to use them, and everybody who needs to be involved in quality assurance must have one. This may work internally, but if algorithms are published and independently audited, the audience will also need access to the proprietary tools. This is not always possible. 3.3 RAP Levels align with AQUA Levels work in progess "],
["levels-of-reproducible-analysis.html", "Chapter 4 Levels of reproducible analysis", " Chapter 4 Levels of reproducible analysis Building a reproducible analytical pipeline (which we will shorten to RAP) can be viewed as a layered process. This section of the guidance will go through many of the steps. You might start off with a basic coding project and add additional levels of enhancement to the project as you go. This layered approach gives a framework for progressing through the different levels of sophistication that are possible - and deciding whether it is appropriate to make the project more elaborate. How far you get will depend on the skill level in your team, the infrastructure available in your department and pragmatic choices about whether the final stages are proportionate to assure fitness for use. For smaller pieces of work, the most sophisticated levels might be too time consuming to put in place, with minimal benefit. However for regular work that you are going to repeat, the more levels you can build in, the more assurance you will have that your code is doing what you expect it to do and that your outputs are as you intended. Deciding how far to go should be pragmatic and proportionate. Think about the minimal level that you need to achieve before your code is put into production. Many projects consists of one or more ad hoc scripts to do a number of steps such as data validation or producing tables. This is level one of the RAP process. The second step is to organize your code into a clear structure. The layout will depend on the project and the tools, but in general this involves separating the code into sections that do particular tasks such as modifying the data or running validation checks. This is an opportunity to start applying coding standards if you are not following them, and that will make the process easier to follow. The next step is to add version control. Version control software such as git and cloud-based code repositories such as GitHub or Gitlab, allow changes to the code to be made collaboratively and recorded. In addition, changes can be rolled back to a previous state. This provides reassurance in that any change can be reversed allowing new methods or approaches to be tried without fear that this will break the current process. This automatically builds documentation into the production process and creates a record of what has been done and why which can be audited in future. This is covered in depth in chapter 5 on GIT. Another step is to re write the code into reusable functions. Functions are organised, reusable pieces of code that perform a specific task. Functions help to keep code clean and allow code re-usability. They can be given useful and informative names and stored separately, making workflows clean and easy to follow. Using functions helps to eliminate mistakes when copying and pasting. More importantly, their use means that when you need to add new functionality you only need to change a single function and those changes will cascade to wherever the function is called. One of the difficulties that can arise in more manual methods of production is that there are many files separated out over different locations, all relating to different but interdependent stages of the process. Often, the process itself requires the use of several tools, such as Excel, SAS and Microsoft Word. Each part of the process needs to be documented and kept up to date, and, in the worst cases, they are only connected through manual intervention. A third step in the process ofo building a reproducible pipeline is to package your code. Packaging brings together all of the scripts that run the pipeline, together with documentation, tests and dummy data that can be used to verify that everything is working. Packaging can gives the project a defined structure. Because the documentation is part of the package and closely intertwined with the code which aids in institutional knowledge transfer. In R, the fundamental unit of shareable code is the package. A package bundles together code, data, documentation, and tests, and is easy to share with others. - Hadley Wickham We recommend that testing should be implemented as part of the process of building an analytical pipeline, when this is pragmatic and proportionate. Chapter 4, Testing, covers this in more detail. The final stages of the RAP process, levels six and seven, are about ensuring that the package is fully tested, that tests are integrated into the version control system and building in functionality to ensure that all tests are run automatically. This improves resilience. Tests are run automatically, and failures are notified automatically on the version control site. Code coverage statistics let users know what percentage of the code (and data) in the package is covered by tests. Public Health &amp; Intelligence Reproducible Analytical Pipelines "],
["Testing.html", "Chapter 5 Testing 5.1 Advantages of testing your code 5.2 When a project should use testing 5.3 When a project might not deploy testing", " Chapter 5 Testing When you are working on a project how do you know that your code is doing what you expect it to do? You run it and look at the output. This is a manual test and it has it’s limitations. Hadley Wickham in The R Journal it’s not that we don’t test our code, it’s that we don’t store our tests so they can be re-run automatically. Manual tests need to be constantly re-run. This is achievable when your code is small but when it becomes longer this is time consuming. When you come back to your code in the future you will have forgotten what tests you have run, any changes are then liable to break something. Fundamentally if you don’t test your code then how do you know that your code is doing what it is suppose to? Just as data validation in an important step in ensuring that the the data is correct, code validation using testing is important to check that the process is correct. 5.1 Advantages of testing your code Better Code Structure Writing tests forces you to separate code into functions that are separate and isolated, reducing duplication in your code. This approach will make your code easier to test and also easier to understand and use. Less Bugs Like double entry book keeping, the behavior of your code is specified twice, once in the code and again in a test which makes it less error prone. More Robust Code If all the major functions of your code have associated tests then code can be changed without the issue that this has caused a break or bug elsewhere which might not be discovered until later. Decreased Frustration Untested code is precarious, an making changes without knowing if you have broken a section is stress inducing. Being able to deploy code that you are confident will work because it is tested will make the process more productive. 5.2 When a project should use testing In a Analytics pipeline -In general if code is planned to be reused then it needs to be tested to ensure that it is working properly. In important pieces of work such as a National Statistic or in modeling work -It is important that National Statistics and models are correct and that the public has confidence in the validity of published work, testing is important to ensure that the code is working as expected and prevents errors which increases confidence in government analysis. When working in a team -Testing ensures that collaborators can alter code without the issue with breaking the code without realizing. This allows a team to work on separate sections and have confidence that they can make changes. 5.3 When a project might not deploy testing One off pieces of work -The effort of writing tests might not be proportionate if a project is onetime. The QA process would then consist of manual checks which might be sufficient. Exploratory work -For exploratory work, where it is unclear whether it will be converted into permanent project then testing may not be appropriate. There would be a benefit to testing if a exploratory project turns into a permanent one, however the code may be changed so fundamentally that all the tests would have to be re written. Fundamentally it is a trade off. "],
["testing-frameworks.html", "Chapter 6 Testing Frameworks 6.1 What exactly is a test 6.2 Unit Test 6.3 Integration Tests 6.4 What to tests 6.5 References", " Chapter 6 Testing Frameworks 6.1 What exactly is a test Testing is just checking that the expected result is the same as the actual result. Automated testing is moving from doing this in a informal ad-hoc way to an automated process which is repeatable. Tests should be Fast Independent Repeatable (deterministic) Self-validating (no manual steps) Thorough (How much do you trust they cover everything?) There are common testing frameworks which make testing much simpler. For R Testthat For python pytest 6.2 Unit Test Unit testing is the cornerstone of modern software development. The purpose is to validate that each unit of the software performs as designed. A unit is the smallest testable part, such as a function. A unit test might check that for a given input you get a certain output. This gives assurance that for the tested cases your function will preform as expected. This might be checking that your code preforms as expected when given a missing value, or a negative value. If all tests are passing then all of the cases that are tested for will work. This removes a lot of the burden of review. Unit testing increases confidence in maintaining or changing code. This is because a test which catch if a change has introduced an error quickly. This also encourages good practice as to make unit testing possible code will need to be modular making it easier to reuse. 6.3 Integration Tests Unit tests ensure that the units of code works as expected, integration tests ensure that the units work together. An example is testing to ensure that a series of functions together do what is expected. In R and Python this can be achieved using the same framework as unit tests. There is a question about how much of your code should you test. This depends on the complexity of system and the importance. For statistics QAAD can be used as framework to gauge the appropriate level of testing. For modeling the AQUA book gives a framework for the appropriate level of QA. 6.4 What to tests A QA engineer walks into a bar. Orders a beer. Orders 0 beers. Orders 99999999999 beers. Orders a lizard. Orders -1 beers. Orders a ueicbksjdhd… What tests should be written to ensure that your code is preform as expected will vary depending on need, output and specific circumstances. There are some general principles to testing which will cover alot of common issues. Usual Data types Checking that code works with expected data but also unexpected data types. Common checks would be for positive/negative values, missing values (NA/NaN), 0, very large numbers, words when there should be numbers and many more. This might involve checking that when given certain values the code doesn’t do something, such as produce a value when given an illogical input. Application Program Interface, API This involves testing the interface to the code, which is where the user will interact with the code. As you are unable to control what they input this is a common source of errors. Output For a given input, what is the function expected to return such as a data table, a list, a number etc. What kind of data structure the function is supposed to return (to make sure other functions work, if it expects a specific data structure). Boundary Value Analysis &amp; Equivalence Partitioning This is an approach to reduce the total number of test cases to a finite set of testable test cases. It is a useful approach to making testing a structured method rather than a series of ad-hoc tests which may not cover important sources of error. Boundary analysis involves testing the extreme ends or boundaries between partitions of the input values. Equivalence Partitioning divides the input data into different equivalence data classes which can each be tested covering the full spectrum of possible values. An example is illustrative. The chart below shows an example of what to test for student grades, with certain values falling into grade classes, such as 70-79 being awarded a C grade. It would be time consuming to test every number so a Boundary Value Analysis &amp; Equivalence Partitioning approach is taken. We want to test that a number in the grade category returns that award and that values below 0 or above 100 are invalid. This reduces the number of tests which need to be written to a pragmatic and proportionate level. 6.5 References Testing Blog Testthat "],
["GIT.html", "Chapter 7 GIT 7.1 Git Introduction 7.2 About Version Control 7.3 What is GIT 7.4 How does GIT actually work 7.5 Typical Git workflow 7.6 Remote Repository 7.7 Branching 7.8 Pull Request and issues", " Chapter 7 GIT What is GIT What is VC How to use GIT ( CLI or GUI) Typical GIT workflow Branching 7.1 Git Introduction Git is a version control system, a tool that tracks changes to your code and shares those changes with others. Git is most useful when combined with GitHub, a website that allows you to share your code with the world, solicit improvements via pull requests and track issues. 7.2 About Version Control Version control is a system that records changes to a file or set of files over time so that you can recall specific versions later. This means that changes can be examined allowing a record to kept of who changed what. The changes can also be reversed, allowing your code to be rolled back to an earlier state. GIT is an immensely popular tool in the data science community for VC. 7.3 What is GIT Git is a version control system, a tool that tracks changes to your code and shares those changes with others. To start a common mistake is to get Git and it’s hosting services mixed up, Git is not GitHub or GitLab, they are websites for hosting projects using git. Git is a Distributed VC, meaning that you will have a local repository which has a special folder inside called .git. You will normally, but don’t have too, have a remote, central repository where you and collaborators can contribute code, this could be on GitHub, GitLab or elsewhere. You and any collaborators have an exact clone of the repository on your local computer. 7.4 How does GIT actually work You are probably familiar with the windows file system. Imagine a folder with some files/data in it. We can make the folder into a Git repository. When we do this Git will keep track of any changes you make to the files, and therefore allow you to see who has changed the files, work with other people on the files and undo any changes. Git thinks of the data as a series of snapshots of a mini file system (the folder). At any time you can take a snapshot, by saving or committing, and git will take a snapshot of what your files look like at that moment, and stores a reference to that snapshot. Git thinks about the data as a stream of snapshots. To actually run Git there are two methods CLI Command Line Interface GUI Graphical User Interface A GUI is effectively a program that will run git for you. However these only implement a subset of git commands and it is generally recommend to run Git on the command line. To do this you need to get a terminal and Git installed. This is easier than it sounds. A popular method is to get Git Bash. This is a terminal interface with Git installed. If you are unfamiliar with running terminal commands there are many of great online resources, and fundamentally you will only need a handful of commands to run what you need. If you don;t want to learn past that you won’t need to. 7.5 Typical Git workflow There are many ways to get a Git repository started. I will discuss turning your folder into a repository. First we need to initialize Git, we can do that with the command git init All git command start with git followed by the command. This will set-up the folder as a git repository, easy! Git won;t actually track any of the files yet, as we haven’t got any snapshots. We do this in two steps. First we add the files we want to be tracked to stage them to be committed. git add . This uses the command add to select the files we want to prepare to track, and . selects all files in the folder. Only do this if you want to track every file! Otherwise replace . with the filename you want such as git add data.csv Now the files are staged for the next commit, we want to take the snapshot. To commit we run git commit -m &quot;a helpful commit comment, anything you want!&quot; This runs the commit command which we take the snapshot. -m tells the commit that we want to leave a message and everything in \"\" is attached as a commit comment. Make this something informative and helpful, as you will want to later look back at these, and others will see them too. That is the basic git workflow for working by yourself. You can then edit the files however you want, e.g in Microsoft word or using R studio, and when you next want to take a snapshot run the command again git add . git commit -m &quot;another informative comment&quot; We can also check on the status of our repository with git status 7.6 Remote Repository Using Git just on your computer is useful for being able to roll back or reverse to previous commits, and for keeping track of what changes you made. But Git is really powerful when combined with a remote repository. This basically creates a clone or identical copy of your git repository (all the files in it) and hosts it on another computer, probably on a website such as GitHub or Gitlab. This allows others to also make a copy of the code, to modify and share there changes with you whilst git keeps track. This is a useful tool for peer review and collaborator coding. it is also a backup if your laptop dies or is lost, as the data is safely stored elsewhere too. To do this is simple. First you need to setup a repository on a hosting site. Follow the instructions on GitHub or elsewhere on how to do this, it is not difficult. However make it an empty repository, no README file. Next we need to connect our git repository to the remote repository. Open a Shell and run: git remote add origin https://github.com/username/reponame This command is a bit more complex but you will only need to run it once. remote add is telling git to add a remote repository to the current git. origin will be the name of the remote repository. https://github.com/username/reponame is the URL of the repository. You will be able to on GitHub or GitLab copy and paste this into the terminal window. Now we have a remote repository added we can push our code to it. The typical workflow is the same but with an added step. First we add our files to be staged, we then commit them taking a snapshot of what the current folder looks like, and then we push this snapshot to the remote repository. git add . git commit -m &quot;helpful and informative commit comment&quot; git push Now if you go to the website you will be able to see your code. If a collaborator now wants to get an exact copy of code they can clone the repository. The folder will be copied onto their computer. git clone https://github.com/username/reponame They run this command on their computer and they will now have the files and you can both work together by following the regular workflow. At this stage there will be your repository, a remote repository on a website and a third repository on a collaborator laptop. 7.7 Branching Branching and merging is what makes Git so powerful and for what it has been optimized for, being a distributed version control system (VCS). Branching is one of the key features of Git. It allows you to diverge from the main lien of development. You might want to do this to run an experiment which you are not sure will work, such as rewriting a section of the code, which you will incorporate only if it succeeds. Yo may create a branch for new features, which are siloed until they are ready to be incorporated. Branching allows you to control the layout of your project and it is a useful tool for working collaboratively as a project can be broken into sections which are worked on branches and incorporated when ready. You can switch back and forth to the original “master branch” or to any other branch whenever is needed. When you work using Git you will always be on a branch. by default when you setup Git it automatically creates a “master” branch. If you want to develop a new feature or section you can easily create a new branch: git branch new_branch_name git checkout new_branch_name branch followed by a name will create a branch with that name and checkout followed by a name of an existing branch will switch to it. If you want to fold the code on a branch into another to combine the project onto branch this can be done by merging. To fold the code on the new branch into he master branch then you need to move onto the master branch and then merge. git checkout master git merge new_branch_name 7.8 Pull Request and issues Pull requests let you tell others about changes you’ve pushed to a branch in a repository on GitHub or GitLab. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch. This is a powerful tool for managing a project and for peer review of code. Team members can work on code on a branch and when they are done open a pull request to merge the branch into the main master branch. Another team member will review this code and can either agree to merge, or post comments on potential improvements. This back and forth ensures errors are found early and in general improves the quality of the project code. This process ensures on high quality code makes it into production. Issues are another method of discussing your code and are a great way to keep track of tasks, bugs or enchantments. For example if there is a section of messy, hard to read code then a issue could be left to rewrite that section. The issue could be general or assigned to an individual with a due date. This is another useful method for managing a coding project. "],
["Coding-Guidelines.html", "Chapter 8 Coding Guidelines 8.1 Why Useful 8.2 Best Practice: Coding Principles 8.3 Style Guides", " Chapter 8 Coding Guidelines Best practices are more about what code you write than how you write it. For example, there are many ways to access databases, but one particular way might be considered ‘best practice’ so that everyone in a team knows how it works and doesn’t have to learn other methods. Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread. – The Tidyverse Style Guide by Hadley Wickham 8.1 Why Useful Coding guidelines are helpful Peer Review Makes it easier to learn Often people new to coding want some guidelines as this makes it easier to learn. It is hard to know what is best practice, so some guidelines can be helpful. Makes maintaining code easier Just as important as writing the code is maintaining it in production, perhaps after the writers have left. Maintainability is always an important consideration to make projects sustainable. Makes projects more coherent If code is written by a team then if everyone is writing in unique distinct styles then the project will be much harder to read when combined. 8.2 Best Practice: Coding Principles Specific and prescription coding guidelines ensure that all the code follows a similar style but can be difficult to implement. However there are some principles which apply to all languages and can be used to improve the quality of your code. Naming The names of variables &amp; functions need to be descriptive (intention-revealing), pronounceable and need to reflect what a function does and what the variables are. Short vs Readability Being efficient with writing does not always lead to a code that is readable, don’t sacrifice readability to make your code shorter. Distinct (Ubiquitous language) Make meaningful distinctions. Do not use the same name for two (or more) distinct items. Use the same term in the code, in the documents and when talking to users. eg Do not use cancer name for the Data and the model. Use cancerData and cancerModel for the Data and model respectively. The same is true for Dataframes and columns of the Dataframes. Code can be self-documenting. Put the effort to improve, organize and structure your code i.e. clean it. Keep functions simple Keep as few arguments as possible when creating a function. A function should aim to do one thing well, if there are lots of arguments then maybe that is a sign you should think about separating the function into multiple functions. The idea it to keep the code together that can get damaged for the same reason of change, when changing something you want to impact as few things as possible. Nesting. You should always avoid nesting as it make the code harder to read and understand. 8.3 Style Guides There are many style guides out there. You may choose to adopt one or part of one. Your team or department could also write some guidelines tailored to your needs to encourage best practice. Here are some existing guidelines: Git Style Guide Best Practice in Programming for Data Scientists: Google’s R Style Guide TidyVerse Style Guide The State of Naming Conventions in R PEP 8 – Style Guide for Python Code Ministry of Justice Analytical Services Coding Standards Data Science Campus Coding Standards Health and Social Care Scotland R Resources "],
["documentation.html", "Chapter 9 Documentation 9.1 Commenting 9.2 README 9.3 Package Documentation 9.4 Markdown for desk notes", " Chapter 9 Documentation An important part of producing analysis is documenting the process. This can be done in many ways but a common issue is that documentation isn’t thorough enough. Understanding how a process was done in the past may rely on partial documentation or the expert knowledge of one or two individuals, which is very risky if they leave. New tools open up different avenues for collecting documentation in ways which might be easier and better. This is not a panacea, but my making documentation and code closely intertwined it encourages the process. 9.1 Commenting This is the process of in a code script putting comments or documentation in with the code. In R and Python this can be done by placing a #, and that line will now be a comment rather than active code. An example is the following function which is not clear what it does by looking. even = function(x){ data = x %% 2 == 0 sum(data) } By applying good coding standards and commenting it is far clearer what the function does, however being more verbose can make the script more cluttered. The aim to is to make the script understandable to your future self and others, and comments are an important element along with good coding standards # A function that returns the number of even numbers in a list find_number_even &lt;- function(x){ # Find the even numbers as TRUE or FALSE find_evens &lt;- x %% 2 == 0 # sum the list to find total number of even sum(find_evens) } 9.2 README This sections assumes you are using GIT. Every project using Git should have a README file. This is a file that is displayed when people view your repository and it is often he first thing people see when they view your project. You can use it to tell people what your project is, what it can do and how people can use it. What you include in it might depend on what stage of development you are at. By summarising the project and including details in acts as a guide to the project for others to use. There are some standards of what to include such as READMEs for GOV.UK and GitHub Guide to README. Some useful information to include is What the project does Why the project is useful How users can get started with the project Where users can get help with your project Who maintains and contributes to the project Departments might want to consider have a README template to standardize what is included across the department. 9.3 Package Documentation A package enshrines all the business knowledge used to create a piece of work in one place, including the code used to generate the output and the documentation. If you bundle your code into a package then functions and other aspects of the project can have associated documentation. This allows a package to be shared with the documentation of how to use the functions built into the package. # For R ?your_package_function help(your_package_function) # For Python help() 9.4 Markdown for desk notes In R and other languages markdown and variants can be used to write desk notes. These are documentation of what an analyst needs to do, such as produce a statistic or generate an output from a model. Rather than produce these in a text editor such as word these could be produced in R markdown or a Jupiter Notebook (both support many languages). This allows blocks of texts describing what, why and how next to live code that can be run to give an example. Documenting the work flow in this manner is another method of interweaving commentary and code together. "],
["standards.html", "Chapter 10 Standards 10.1 Standards for Peer review 10.2 Standard Structures for Projects 10.3 Dependency Management", " Chapter 10 Standards 10.1 Standards for Peer review An important element of getting quality code is peer review. Showing people your code and getting it scrutinized is an essential part of ensuring that code is doing what it is supposed to do. It also allows for iteration, a process where the back and forth of reviewing code cleans and sharpens it. This process is made easier by coding standards. When code is written in a familiar style it will be easier for others to pick it up quickly and understand what was your intention. Poorly written code may achieve the desired output but is usually difficult to read and follow, making it hard to scrutinise and quality assure. This increases the likelihood that it will contain errors or have unforeseen side effects. A common language aids this as a business area which uses Excel, R, SAS, SQL, Scala, SPSS and Python will struggle to maintain effective peer review because of the number of different coding platforms in use. It is advantageous to consider a wider peer review, where people outside your team review your code. This can be challenging, depending on the systems in use and the sensitivity of your data. However code itself is usually not sensitive, and it is recommended that code is shared and scrutinised widely to increase robustness and promote transparency. Using dummy data or synthetic data which has the same structure and patterns as the actual data but has been randomly generated is one way to enable peer review from those who do not have access to the data. Departments or business areas might find it useful to have standards for peer review. This could involve a checklist of common things issues or a procedure to follow. An example is provided by ropensci. A peer review template might be useful for encouraging a unified approach and ensuring that important checks don’t get missed. 10.2 Standard Structures for Projects For projects of a similar type a standard structure might be useful. This would allow users to pick up projects quickly as they would have a general idea of the layout. For example you can look at Cookiecutter Data Science. This would aid in peer review as it would be possible to get analysts outside of your team to review your code, without them having to understand a unique structure. It might be that there is a better good practice structure that departments could encourage by having or adopting a standard. It would also allow projects to start faster as the standard could be copied, and then modified to fit the project as needed. 10.3 Dependency Management Packrat / Docker "]
]
